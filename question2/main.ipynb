{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from functions import *\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "mat = scipy.io.loadmat('face.mat')\n",
    "raw_data = mat['X']\n",
    "\n",
    "D,N = raw_data.shape\n",
    "\n",
    "raw_data = np.transpose(raw_data)\n",
    "\n",
    "partitioned_training_data = np.empty([4,int(520*0.8/4), 2576])\n",
    "\n",
    "testing_data = np.empty([int(520*0.2), 2576])\n",
    "\n",
    "# create training and test data\n",
    "for x in range(52):\n",
    "    for y in range(4):\n",
    "        partitioned_training_data[y][x*2:(x+1)*2] = raw_data[x*10+(2*y):x*10+(2*(y+1))]\n",
    "    testing_data[x*2:(x+1)*2] = raw_data[x*10+8:(x+1)*10]\n",
    "    \n",
    "\n",
    "raw_data = np.transpose(raw_data)\n",
    "# partitioned_training_data = np.transpose(partitioned_training_data)\n",
    "testing_data = np.transpose(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = partitioned_training_data[0]\n",
    "# two = partitioned_training_data[1]\n",
    "# training_data_now = np.concatenate((one,two), axis=0)\n",
    "# print(training_data_now.shape)\n",
    "# mean = np.empty([4,2576])\n",
    "\n",
    "# for i in range(4):\n",
    "#     temp0 = partitioned_training_data[0]\n",
    "#     temp1 = partitioned_training_data[1]\n",
    "#     temp2 = np.concatenate((temp0,temp1), axis=0)\n",
    "#     mean[i] = temp2.mean(axis=0)\n",
    "    \n",
    "# print(mean[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute covariance matrix and mean of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = np.empty([4,416,416])\n",
    "# D = np.empty([4])\n",
    "# N = np.empty([4])\n",
    "# A = np.empty([4,training_data_now,2576])\n",
    "\n",
    "# for i in range(4):\n",
    "#     A[i] = training_data_now - mean[i]\n",
    "#     D[i],N[i] = A[i].shape\n",
    "#     S[i] = (1/N[i])*np.dot(A[i],A[i].T) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only 1 partition of the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine partition 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds0 :  (104, 2576)\n",
      "ds1 :  (104, 2576)\n",
      "s0 :  (2576, 2576)\n",
      "combined_cov - combined_cov :  (2576, 2576)\n",
      "v0 shape :  (2576, 104)\n",
      "v1 shape :  (2576, 104)\n",
      "mu diff shape :  (2576,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d3dddfaa3281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ds1 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcombined_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigenface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/imperial/year4/pattern_recogn/pattern_recog_cw1/question2/functions.py\u001b[0m in \u001b[0;36mmerge_dataset\u001b[0;34m(ds0, ds1)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v1 shape : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mu diff shape : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"phi shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "# # new_mu = (N[0]*mean[0] + N[1]*mean[1])/(N[0]+N[1])\n",
    "# # new_cov = (N[0]/(N[0]+N[1]))*S[0] + (N[1]/(N[0]+N[1]))*S[1] + ((N[0]*N[1])/(N[0]+N[1])**2)*np.dot(mean[0]-mean[1],(mean[0]-mean[1]).T)\n",
    "# new_mu = combined_mean(N,mean,0,1)\n",
    "# new_cov = combined_cov(N,S,mean,0,1)\n",
    "\n",
    "# q,r = LA.qr(new_cov)\n",
    "\n",
    "# p = np.matmul(q,r)\n",
    "# w,v = LA.eig(new_cov)\n",
    "\n",
    "# u = np.matmul(A[0].T,v)\n",
    "\n",
    "# u /= LA.norm(u,ord=2,axis=0)\n",
    "# id = np.argsort(np.abs(w))[::-1]\n",
    "# w = w[id]\n",
    "# u = u[:,id].real\n",
    "# # show_img(u[:,0])\n",
    "ds0 = partitioned_training_data[0]\n",
    "ds1 = partitioned_training_data[1]\n",
    "print(\"ds0 : \",ds0.shape)\n",
    "print(\"ds1 : \",ds1.shape)\n",
    "\n",
    "combined_training, eigenface, new_mu, new_cov = merge_dataset(ds0, ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face = new_mu.reshape(-1,1)\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = reconstruct(testing_data, new_mu, eigenface, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    show_img(faces[i,:])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the reconstruction error and face recognition accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_err(combined_training, testing_data, new_mu, eigenface, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Incremental PCA, and compare it with the counterpart i.e. :\n",
    "    - batch PCA\n",
    "    - PCA trained only by the first subset\n",
    "    \n",
    "in terms of :\n",
    "    - training time\n",
    "    - reconstruction error\n",
    "    - face recognition accuracy.\n",
    "    \n",
    "Show and discuss, including: how accurate your incremental method is, what important\n",
    "parameters in the method are (and how they are set). Provide your own discussions and\n",
    "measurements to support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
